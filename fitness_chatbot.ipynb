{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nComplete Setup and Installation Script for Fitness Q&A Model Training\nThis script handles all installations and imports needed for the project\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef install_package(package_name, upgrade=False):\n    \"\"\"Install a package using pip\"\"\"\n    try:\n        cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n        if upgrade:\n            cmd.append(\"--upgrade\")\n        cmd.append(package_name)\n        \n        print(f\"Installing {package_name}...\")\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        print(f\"âœ“ Successfully installed {package_name}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âœ— Failed to install {package_name}: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\ndef check_and_install_packages():\n    \"\"\"Check and install all required packages\"\"\"\n    \n    # Core packages with specific versions for compatibility\n    # Note: PyArrow version is critical for datasets compatibility\n    packages = [\n        \"torch>=2.0.0\",\n        \"pyarrow>=12.0.0,<18.0.0\",  # Fix PyArrow compatibility issue\n        \"transformers>=4.30.0\",\n        \"datasets>=2.12.0\",\n        \"pandas>=1.5.0\",\n        \"numpy>=1.21.0\",\n        \"scikit-learn>=1.2.0\",\n        \"evaluate>=0.4.0\",\n        \"nltk>=3.8.0\",\n        \"accelerate>=0.20.0\",  # For better training performance\n        \"sentencepiece>=0.1.99\",  # Required for T5 tokenizer\n        \"protobuf>=3.20.0\",  # Required for datasets\n        \"tqdm>=4.64.0\",  # Progress bars\n        \"requests>=2.28.0\",  # For downloading datasets\n        \"huggingface_hub>=0.15.0\",  # For model/dataset downloads\n    ]\n    \n    print(\"=\" * 60)\n    print(\"INSTALLING REQUIRED PACKAGES\")\n    print(\"=\" * 60)\n    \n    failed_packages = []\n    \n    # First, upgrade pip itself\n    print(\"Upgrading pip...\")\n    try:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], \n                      check=True, capture_output=True)\n        print(\"âœ“ pip upgraded successfully\")\n    except:\n        print(\"âš  Could not upgrade pip, continuing anyway...\")\n    \n    # Special handling for PyArrow compatibility issue\n    print(\"\\nðŸ”§ Fixing PyArrow compatibility issue...\")\n    try:\n        # Uninstall potentially conflicting PyArrow versions\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], \n                      capture_output=True)\n        # Install compatible version\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow>=12.0.0,<18.0.0\"], \n                      check=True, capture_output=True)\n        print(\"âœ“ PyArrow compatibility fixed\")\n    except Exception as e:\n        print(f\"âš  Could not fix PyArrow: {e}\")\n    \n    # Install packages\n    for package in packages:\n        if not install_package(package):\n            failed_packages.append(package)\n    \n    # Install optional packages for better performance\n    optional_packages = [\n        \"tensorboard\",  # For monitoring training\n        \"matplotlib\",   # For plotting if needed\n        \"seaborn\",      # For better plots\n    ]\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"INSTALLING OPTIONAL PACKAGES\")\n    print(\"=\" * 60)\n    \n    for package in optional_packages:\n        install_package(package)  # Don't track failures for optional packages\n    \n    if failed_packages:\n        print(f\"\\nâš  Warning: Failed to install: {', '.join(failed_packages)}\")\n        print(\"You may need to install these manually or check your environment.\")\n    else:\n        print(\"\\nâœ“ All required packages installed successfully!\")\n    \n    return len(failed_packages) == 0\n\ndef fix_pyarrow_issue():\n    \"\"\"Fix the specific PyArrow compatibility issue\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FIXING PYARROW COMPATIBILITY ISSUE\")\n    print(\"=\" * 60)\n    \n    try:\n        # Step 1: Uninstall problematic packages\n        packages_to_remove = [\"pyarrow\", \"datasets\"]\n        for package in packages_to_remove:\n            try:\n                subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package], \n                              capture_output=True, check=False)\n                print(f\"âœ“ Uninstalled {package}\")\n            except:\n                print(f\"âš  Could not uninstall {package} (may not be installed)\")\n        \n        # Step 2: Install compatible PyArrow first\n        compatible_pyarrow = \"pyarrow>=12.0.0,<15.0.0\"\n        if install_package(compatible_pyarrow):\n            print(\"âœ“ Installed compatible PyArrow version\")\n        else:\n            print(\"âœ— Failed to install compatible PyArrow\")\n            return False\n        \n        # Step 3: Install datasets with the compatible PyArrow\n        if install_package(\"datasets>=2.12.0\"):\n            print(\"âœ“ Installed datasets with compatible PyArrow\")\n        else:\n            print(\"âœ— Failed to install datasets\")\n            return False\n        \n        # Step 4: Test the fix\n        try:\n            import pyarrow\n            import datasets\n            print(f\"âœ“ PyArrow version: {pyarrow.__version__}\")\n            print(f\"âœ“ Datasets version: {datasets.__version__}\")\n            \n            # Test basic functionality\n            from datasets import Dataset\n            test_data = {\"text\": [\"hello\", \"world\"], \"label\": [1, 0]}\n            test_dataset = Dataset.from_dict(test_data)\n            print(\"âœ“ Datasets functionality test passed\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"âœ— Test failed: {e}\")\n            return False\n            \n    except Exception as e:\n        print(f\"âœ— PyArrow fix failed: {e}\")\n        return False\ndef verify_installations(skip_datasets=False):\n    \"\"\"Verify that all packages can be imported correctly\"\"\"\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"VERIFYING INSTALLATIONS\")\n    print(\"=\" * 60)\n    \n    imports_to_test = [\n        (\"torch\", \"PyTorch\"),\n        (\"transformers\", \"Transformers\"),\n        (\"pandas\", \"Pandas\"),\n        (\"numpy\", \"NumPy\"),\n        (\"sklearn\", \"Scikit-learn\"),\n        (\"evaluate\", \"Evaluate\"),\n        (\"nltk\", \"NLTK\"),\n        (\"accelerate\", \"Accelerate\"),\n    ]\n    \n    # Add datasets only if PyArrow was fixed\n    if not skip_datasets:\n        imports_to_test.append((\"datasets\", \"Datasets\"))\n    \n    failed_imports = []\n    \n    for module_name, display_name in imports_to_test:\n        try:\n            __import__(module_name)\n            print(f\"âœ“ {display_name} imported successfully\")\n        except ImportError as e:\n            print(f\"âœ— Failed to import {display_name}: {e}\")\n            failed_imports.append(display_name)\n        except AttributeError as e:\n            if \"pyarrow\" in str(e).lower():\n                print(f\"âœ— {display_name} failed due to PyArrow compatibility: {e}\")\n                failed_imports.append(display_name)\n            else:\n                print(f\"âœ— Failed to import {display_name}: {e}\")\n                failed_imports.append(display_name)\n    \n    # Special checks\n    try:\n        import torch\n        print(f\"âœ“ PyTorch version: {torch.__version__}\")\n        print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            print(f\"âœ“ CUDA device count: {torch.cuda.device_count()}\")\n    except:\n        print(\"âœ— Could not get PyTorch details\")\n    \n    try:\n        import transformers\n        print(f\"âœ“ Transformers version: {transformers.__version__}\")\n    except:\n        print(\"âœ— Could not get Transformers version\")\n    \n    if not skip_datasets:\n        try:\n            import pyarrow\n            import datasets\n            print(f\"âœ“ PyArrow version: {pyarrow.__version__}\")\n            print(f\"âœ“ Datasets version: {datasets.__version__}\")\n        except:\n            print(\"âœ— Could not get PyArrow/Datasets versions\")\n    \n    if failed_imports:\n        print(f\"\\nâš  Warning: Failed to import: {', '.join(failed_imports)}\")\n        return False\n    else:\n        print(\"\\nâœ“ All packages verified successfully!\")\n        return True\n\ndef download_nltk_data():\n    \"\"\"Download required NLTK data\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"DOWNLOADING NLTK DATA\")\n    print(\"=\" * 60)\n    \n    try:\n        import nltk\n        \n        # Download required NLTK data\n        nltk_downloads = [\n            'punkt',\n            'stopwords',\n            'wordnet',\n            'omw-1.4'\n        ]\n        \n        for item in nltk_downloads:\n            try:\n                nltk.download(item, quiet=True)\n                print(f\"âœ“ Downloaded NLTK {item}\")\n            except Exception as e:\n                print(f\"âš  Could not download NLTK {item}: {e}\")\n                \n    except ImportError:\n        print(\"âœ— NLTK not available for downloading data\")\n\ndef setup_environment():\n    \"\"\"Set up environment variables and create directories\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SETTING UP ENVIRONMENT\")\n    print(\"=\" * 60)\n    \n    # Set environment variables\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/transformers\"\n    os.environ[\"HF_DATASETS_CACHE\"] = \"./cache/datasets\"\n    print(\"âœ“ Environment variables set\")\n    \n    # Create necessary directories\n    directories = [\n        \"cache\",\n        \"cache/transformers\", \n        \"cache/datasets\",\n        \"processed_data\",\n        \"models\",\n        \"logs\"\n    ]\n    \n    for directory in directories:\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        print(f\"âœ“ Created directory: {directory}\")\n\ndef main():\n    \"\"\"Main setup function\"\"\"\n    print(\"ðŸš€ Starting complete setup for Fitness Q&A Model Training\")\n    print(\"This may take several minutes...\")\n    \n    # Step 0: Fix PyArrow issue first (critical fix)\n    print(\"\\nðŸ”§ Applying critical compatibility fixes...\")\n    pyarrow_fixed = fix_pyarrow_issue()\n    \n    # Step 1: Install packages\n    packages_ok = check_and_install_packages()\n    \n    # Step 2: Verify installations (skip datasets if PyArrow fix failed)\n    imports_ok = verify_installations(skip_datasets=not pyarrow_fixed)\n    \n    # Step 3: Download NLTK data\n    download_nltk_data()\n    \n    # Step 4: Setup environment\n    setup_environment()\n    \n    # Final status\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SETUP COMPLETE\")\n    print(\"=\" * 60)\n    \n    if packages_ok and imports_ok:\n        print(\"âœ… Setup completed successfully!\")\n        print(\"You can now run the fitness Q&A training script.\")\n        \n        # Test basic functionality\n        print(\"\\nðŸ§ª Running quick functionality test...\")\n        try:\n            import torch\n            from transformers import AutoTokenizer\n            \n            # Test tokenizer loading\n            tokenizer = AutoTokenizer.from_pretrained('t5-small')\n            print(\"âœ“ T5 tokenizer loads correctly\")\n            \n            # Test basic tokenization\n            test_text = \"question: How can I improve my fitness?\"\n            tokens = tokenizer(test_text, return_tensors=\"pt\")\n            print(\"âœ“ Tokenization works correctly\")\n            \n            # Test datasets if PyArrow was fixed\n            if pyarrow_fixed:\n                from datasets import Dataset\n                test_data = {\"question\": [\"test?\"], \"answer\": [\"test answer\"]}\n                test_ds = Dataset.from_dict(test_data)\n                print(\"âœ“ Datasets functionality works correctly\")\n            \n            print(\"âœ… All functionality tests passed!\")\n            \n        except Exception as e:\n            print(f\"âš  Functionality test failed: {e}\")\n            print(\"You may need to restart your Python session.\")\n    \n    else:\n        print(\"âŒ Setup completed with some issues.\")\n        print(\"Please check the error messages above and resolve any missing packages.\")\n        if not pyarrow_fixed:\n            print(\"\\nðŸ”§ PyArrow compatibility issue detected!\")\n            print(\"Try running this fix manually:\")\n            print(\"pip uninstall -y pyarrow datasets\")\n            print(\"pip install 'pyarrow>=12.0.0,<15.0.0'\")\n            print(\"pip install 'datasets>=2.12.0'\")\n    \n    print(\"\\nðŸ“‹ Next steps:\")\n    print(\"1. If setup was successful, you can now run the main training script\")\n    print(\"2. If there were issues, please install missing packages manually\")\n    print(\"3. Consider restarting your Python kernel/session after installation\")\n    print(\"4. If PyArrow issues persist, try the manual fix commands above\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n# =============================================================================\n# IMPORTS SECTION - All imports needed for the main script\n# =============================================================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTING ALL IMPORTS FOR MAIN SCRIPT\")\nprint(\"=\" * 60)\n\ntry:\n    # Standard library imports\n    import os\n    import re\n    import logging\n    from pathlib import Path\n    from typing import Dict, List, Optional, Tuple\n    print(\"âœ“ Standard library imports successful\")\n    \n    # Data handling\n    import pandas as pd\n    import numpy as np\n    print(\"âœ“ Data handling imports successful\")\n    \n    # Machine learning\n    from sklearn.model_selection import train_test_split\n    print(\"âœ“ Scikit-learn imports successful\")\n    \n    # PyTorch\n    import torch\n    print(\"âœ“ PyTorch import successful\")\n    \n    # NLTK\n    import nltk\n    print(\"âœ“ NLTK import successful\")\n    \n    # Hugging Face\n    from datasets import Dataset, load_dataset\n    from transformers import (\n        AutoTokenizer, \n        T5ForConditionalGeneration, \n        Trainer, \n        TrainingArguments,\n        EarlyStoppingCallback\n    )\n    print(\"âœ“ Transformers imports successful\")\n    \n    # Evaluation\n    import evaluate\n    print(\"âœ“ Evaluate import successful\")\n    \n    print(\"\\nâœ… ALL IMPORTS SUCCESSFUL - Ready to run main script!\")\n    \nexcept ImportError as e:\n    print(f\"\\nâŒ Import failed: {e}\")\n    print(\"Please run the setup section above to install missing packages.\")\n\n# =============================================================================\n# SYSTEM INFO\n# =============================================================================\n\ndef print_system_info():\n    \"\"\"Print system information for debugging\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SYSTEM INFORMATION\")\n    print(\"=\" * 60)\n    \n    print(f\"Python version: {sys.version}\")\n    print(f\"Platform: {sys.platform}\")\n    \n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    except:\n        pass\n    \n    try:\n        import transformers\n        print(f\"Transformers version: {transformers.__version__}\")\n    except:\n        pass\n    \n    try:\n        import datasets\n        print(f\"Datasets version: {datasets.__version__}\")\n    except:\n        pass\n\nprint_system_info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T04:33:38.666910Z","iopub.execute_input":"2025-06-18T04:33:38.667568Z","iopub.status.idle":"2025-06-18T04:36:23.278282Z","shell.execute_reply.started":"2025-06-18T04:33:38.667534Z","shell.execute_reply":"2025-06-18T04:36:23.277265Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting complete setup for Fitness Q&A Model Training\nThis may take several minutes...\n\nðŸ”§ Applying critical compatibility fixes...\n\n============================================================\nFIXING PYARROW COMPATIBILITY ISSUE\n============================================================\nâœ“ Uninstalled pyarrow\nâœ“ Uninstalled datasets\nInstalling pyarrow>=12.0.0,<15.0.0...\nâœ“ Successfully installed pyarrow>=12.0.0,<15.0.0\nâœ“ Installed compatible PyArrow version\nInstalling datasets>=2.12.0...\nâœ“ Successfully installed datasets>=2.12.0\nâœ“ Installed datasets with compatible PyArrow\nâœ“ PyArrow version: 20.0.0\nâœ“ Datasets version: 3.6.0\nâœ“ Datasets functionality test passed\n============================================================\nINSTALLING REQUIRED PACKAGES\n============================================================\nUpgrading pip...\nâœ“ pip upgraded successfully\n\nðŸ”§ Fixing PyArrow compatibility issue...\nâœ“ PyArrow compatibility fixed\nInstalling torch>=2.0.0...\nâœ“ Successfully installed torch>=2.0.0\nInstalling pyarrow>=12.0.0,<18.0.0...\nâœ“ Successfully installed pyarrow>=12.0.0,<18.0.0\nInstalling transformers>=4.30.0...\nâœ“ Successfully installed transformers>=4.30.0\nInstalling datasets>=2.12.0...\nâœ“ Successfully installed datasets>=2.12.0\nInstalling pandas>=1.5.0...\nâœ“ Successfully installed pandas>=1.5.0\nInstalling numpy>=1.21.0...\nâœ“ Successfully installed numpy>=1.21.0\nInstalling scikit-learn>=1.2.0...\nâœ“ Successfully installed scikit-learn>=1.2.0\nInstalling evaluate>=0.4.0...\nâœ“ Successfully installed evaluate>=0.4.0\nInstalling nltk>=3.8.0...\nâœ“ Successfully installed nltk>=3.8.0\nInstalling accelerate>=0.20.0...\nâœ“ Successfully installed accelerate>=0.20.0\nInstalling sentencepiece>=0.1.99...\nâœ“ Successfully installed sentencepiece>=0.1.99\nInstalling protobuf>=3.20.0...\nâœ“ Successfully installed protobuf>=3.20.0\nInstalling tqdm>=4.64.0...\nâœ“ Successfully installed tqdm>=4.64.0\nInstalling requests>=2.28.0...\nâœ“ Successfully installed requests>=2.28.0\nInstalling huggingface_hub>=0.15.0...\nâœ“ Successfully installed huggingface_hub>=0.15.0\n\n============================================================\nINSTALLING OPTIONAL PACKAGES\n============================================================\nInstalling tensorboard...\nâœ“ Successfully installed tensorboard\nInstalling matplotlib...\nâœ“ Successfully installed matplotlib\nInstalling seaborn...\nâœ“ Successfully installed seaborn\n\nâœ“ All required packages installed successfully!\n\n============================================================\nVERIFYING INSTALLATIONS\n============================================================\nâœ“ PyTorch imported successfully\nâœ“ Transformers imported successfully\nâœ“ Pandas imported successfully\nâœ“ NumPy imported successfully\nâœ“ Scikit-learn imported successfully\n","output_type":"stream"},{"name":"stderr","text":"2025-06-18 04:36:01.969924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750221362.188675      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750221362.260036      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Evaluate imported successfully\nâœ“ NLTK imported successfully\nâœ“ Accelerate imported successfully\nâœ“ Datasets imported successfully\nâœ“ PyTorch version: 2.6.0+cu124\nâœ“ CUDA available: True\nâœ“ CUDA device count: 1\nâœ“ Transformers version: 4.51.3\nâœ“ PyArrow version: 20.0.0\nâœ“ Datasets version: 3.6.0\n\nâœ“ All packages verified successfully!\n\n============================================================\nDOWNLOADING NLTK DATA\n============================================================\nâœ“ Downloaded NLTK punkt\nâœ“ Downloaded NLTK stopwords\nâœ“ Downloaded NLTK wordnet\nâœ“ Downloaded NLTK omw-1.4\n\n============================================================\nSETTING UP ENVIRONMENT\n============================================================\nâœ“ Environment variables set\nâœ“ Created directory: cache\nâœ“ Created directory: cache/transformers\nâœ“ Created directory: cache/datasets\nâœ“ Created directory: processed_data\nâœ“ Created directory: models\nâœ“ Created directory: logs\n\n============================================================\nSETUP COMPLETE\n============================================================\nâœ… Setup completed successfully!\nYou can now run the fitness Q&A training script.\n\nðŸ§ª Running quick functionality test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6cc56bf0ace4ca398567d8b6f8ce8d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334d2ee5f07845ad9384047b902bce86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87a033cbb0743bf9124c1814a177401"}},"metadata":{}},{"name":"stdout","text":"âœ“ T5 tokenizer loads correctly\nâœ“ Tokenization works correctly\nâœ“ Datasets functionality works correctly\nâœ… All functionality tests passed!\n\nðŸ“‹ Next steps:\n1. If setup was successful, you can now run the main training script\n2. If there were issues, please install missing packages manually\n3. Consider restarting your Python kernel/session after installation\n4. If PyArrow issues persist, try the manual fix commands above\n\n============================================================\nTESTING ALL IMPORTS FOR MAIN SCRIPT\n============================================================\nâœ“ Standard library imports successful\nâœ“ Data handling imports successful\nâœ“ Scikit-learn imports successful\nâœ“ PyTorch import successful\nâœ“ NLTK import successful\nâœ“ Transformers imports successful\nâœ“ Evaluate import successful\n\nâœ… ALL IMPORTS SUCCESSFUL - Ready to run main script!\n\n============================================================\nSYSTEM INFORMATION\n============================================================\nPython version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nPlatform: linux\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nGPU count: 1\nGPU 0: Tesla P100-PCIE-16GB\nTransformers version: 4.51.3\nDatasets version: 3.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nFitness Q&A Model Training Pipeline\nA complete pipeline for training a T5 model on fitness-related question-answer pairs\n\"\"\"\n\nimport os\nimport re\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport torch\nimport nltk\nfrom datasets import Dataset, load_dataset\nfrom transformers import (\n    AutoTokenizer, \n    T5ForConditionalGeneration, \n    Trainer, \n    TrainingArguments,\n    EarlyStoppingCallback\n)\nfrom sklearn.model_selection import train_test_split\nimport evaluate\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress tokenizers parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nclass FitnessQAProcessor:\n    \"\"\"Handles data preprocessing for fitness Q&A dataset\"\"\"\n    \n    FITNESS_KEYWORDS = [\n        'exercise', 'workout', 'fitness', 'nutrition', 'muscle', 'cardio', \n        'strength', 'yoga', 'running', 'sleep', 'stress', 'recovery', \n        'flexibility', 'balance', 'posture', 'hydration', 'motivation',\n        'diet', 'weight', 'training', 'gym', 'health', 'stretch'\n    ]\n    \n    def __init__(self):\n        self.question_col = 'Question'\n        self.answer_col = 'Answer'\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        if not isinstance(text, str) or pd.isna(text):\n            return \"\"\n        \n        # Convert to lowercase and strip whitespace\n        text = text.lower().strip()\n        \n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        \n        # Remove special characters but keep basic punctuation\n        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n        \n        # Remove very short texts (likely noise)\n        if len(text.split()) < 3:\n            return \"\"\n            \n        return text\n    \n    def is_valid_question(self, question: str) -> bool:\n        \"\"\"Check if text is a valid question\"\"\"\n        if not question or len(question) < 10:\n            return False\n        \n        # Must contain letters and proper question structure\n        if not re.search(r'[a-zA-Z]', question):\n            return False\n        \n        # Should end with question mark or be a question word\n        question_indicators = ['how', 'what', 'why', 'when', 'where', 'which', 'who', 'can', 'should', 'do', 'does', 'is', 'are']\n        ends_with_question = question.endswith('?')\n        starts_with_question = any(question.startswith(word) for word in question_indicators)\n        \n        return ends_with_question or starts_with_question\n    \n    def is_fitness_related(self, text: str) -> bool:\n        \"\"\"Check if text is fitness related\"\"\"\n        pattern = '|'.join(self.FITNESS_KEYWORDS)\n        return bool(re.search(pattern, text, re.IGNORECASE))\n    \n    def load_and_clean_data(self, dataset_name: str = \"its-myrto/fitness-question-answers\") -> pd.DataFrame:\n        \"\"\"Load and clean the fitness Q&A dataset\"\"\"\n        try:\n            logger.info(f\"Loading dataset: {dataset_name}\")\n            dataset = load_dataset(dataset_name)\n            df = dataset['train'].to_pandas()\n            logger.info(f\"Initial dataset size: {len(df)}\")\n            \n            # Drop unnecessary columns\n            cols_to_drop = ['Unnamed: 0'] if 'Unnamed: 0' in df.columns else []\n            if cols_to_drop:\n                df = df.drop(columns=cols_to_drop)\n                logger.info(f\"Dropped columns: {cols_to_drop}\")\n            \n            # Verify required columns exist\n            if self.question_col not in df.columns or self.answer_col not in df.columns:\n                raise ValueError(f\"Required columns '{self.question_col}' and/or '{self.answer_col}' not found\")\n            \n            # Clean the data\n            df = self._clean_dataframe(df)\n            \n            logger.info(f\"Final cleaned dataset size: {len(df)}\")\n            return df\n            \n        except Exception as e:\n            logger.error(f\"Error loading dataset: {e}\")\n            raise\n    \n    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply all cleaning steps to dataframe\"\"\"\n        initial_size = len(df)\n        \n        # Remove duplicates\n        df = df.drop_duplicates(subset=[self.question_col, self.answer_col], keep='first')\n        logger.info(f\"Removed {initial_size - len(df)} duplicates\")\n        \n        # Remove missing values\n        df = df.dropna(subset=[self.question_col, self.answer_col])\n        logger.info(f\"Removed rows with missing values, remaining: {len(df)}\")\n        \n        # Clean text\n        df[self.question_col] = df[self.question_col].apply(self.clean_text)\n        df[self.answer_col] = df[self.answer_col].apply(self.clean_text)\n        \n        # Remove empty entries after cleaning\n        df = df[(df[self.question_col] != \"\") & (df[self.answer_col] != \"\")]\n        \n        # Filter valid questions\n        df = df[df[self.question_col].apply(self.is_valid_question)]\n        logger.info(f\"After filtering valid questions: {len(df)}\")\n        \n        # Filter fitness-related content\n        fitness_mask = (df[self.question_col].apply(self.is_fitness_related) | \n                       df[self.answer_col].apply(self.is_fitness_related))\n        df = df[fitness_mask]\n        logger.info(f\"After filtering fitness-related content: {len(df)}\")\n        \n        return df.reset_index(drop=True)\n\nclass T5FitnessTrainer:\n    \"\"\"Handles T5 model training for fitness Q&A\"\"\"\n    \n    def __init__(self, model_name: str = 't5-small', max_length: int = 512):\n        self.model_name = model_name\n        self.max_length = max_length\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.tokenizer = None\n        self.model = None\n        \n        # Download NLTK data for BLEU computation\n        try:\n            nltk.download('punkt', quiet=True)\n        except:\n            logger.warning(\"Could not download NLTK punkt tokenizer\")\n    \n    def initialize_model(self):\n        \"\"\"Initialize tokenizer and model\"\"\"\n        try:\n            logger.info(f\"Initializing {self.model_name} on {self.device}\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n            self.model.to(self.device)\n            logger.info(\"Model and tokenizer initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Error initializing model: {e}\")\n            raise\n    \n    def prepare_dataset(self, df: pd.DataFrame) -> Dataset:\n        \"\"\"Convert DataFrame to tokenized Dataset\"\"\"\n        inputs = [f\"question: {row['Question']}\" for _, row in df.iterrows()]\n        targets = [row['Answer'] for _, row in df.iterrows()]\n        \n        # Tokenize inputs\n        input_encodings = self.tokenizer(\n            inputs,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Tokenize targets\n        target_encodings = self.tokenizer(\n            targets,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Create dataset\n        dataset_dict = {\n            'input_ids': input_encodings['input_ids'],\n            'attention_mask': input_encodings['attention_mask'],\n            'labels': target_encodings['input_ids']\n        }\n        \n        return Dataset.from_dict(dataset_dict)\n    \n    def compute_metrics(self, eval_pred) -> Dict[str, float]:\n        \"\"\"Compute evaluation metrics\"\"\"\n        predictions, labels = eval_pred\n        \n        # Handle model output format\n        if isinstance(predictions, tuple):\n            predictions = predictions[0]\n        \n        # Convert to text\n        if predictions.ndim == 3:\n            predictions = predictions.argmax(-1)\n        \n        pred_texts = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        label_texts = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n        \n        # Remove padding tokens from labels\n        label_texts = [text.replace(self.tokenizer.pad_token, \"\").strip() for text in label_texts]\n        \n        # Compute BLEU score\n        try:\n            bleu = evaluate.load(\"bleu\")\n            result = bleu.compute(\n                predictions=pred_texts, \n                references=[[label] for label in label_texts]\n            )\n            return {\"bleu\": result[\"bleu\"]}\n        except Exception as e:\n            logger.warning(f\"Could not compute BLEU score: {e}\")\n            return {\"bleu\": 0.0}\n    \n    def train_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame, \n                   output_dir: str = \"./fitness_qa_model\") -> None:\n        \"\"\"Train the T5 model\"\"\"\n        if self.model is None or self.tokenizer is None:\n            self.initialize_model()\n        \n        # Prepare datasets\n        logger.info(\"Preparing training datasets...\")\n        train_dataset = self.prepare_dataset(train_df)\n        val_dataset = self.prepare_dataset(val_df)\n        \n        # Training arguments\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=16,\n            warmup_steps=200,\n            weight_decay=0.01,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=50,\n            eval_strategy=\"steps\",\n            eval_steps=200,\n            save_strategy=\"steps\",\n            save_steps=200,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"bleu\",\n            greater_is_better=True,\n            save_total_limit=2,\n            fp16=torch.cuda.is_available(),\n            dataloader_pin_memory=False,\n            report_to=[]  # Disable wandb logging\n        )\n        \n        # Initialize trainer\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            compute_metrics=self.compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        \n        # Train the model\n        logger.info(\"Starting training...\")\n        try:\n            trainer.train()\n            logger.info(\"Training completed successfully\")\n        except Exception as e:\n            logger.error(f\"Training failed: {e}\")\n            raise\n        \n        # Save final model\n        final_output_dir = f\"{output_dir}_final\"\n        trainer.save_model(final_output_dir)\n        self.tokenizer.save_pretrained(final_output_dir)\n        logger.info(f\"Model saved to {final_output_dir}\")\n        \n        return trainer\n    \n    def test_model(self, test_questions: Optional[List[str]] = None) -> None:\n        \"\"\"Test the trained model with sample questions\"\"\"\n        if test_questions is None:\n            test_questions = [\n                \"How can I improve my running endurance?\",\n                \"What are effective core exercises?\",\n                \"How do I stay motivated for workouts?\",\n                \"What should I eat before exercising?\",\n                \"How often should I rest between workouts?\"\n            ]\n        \n        if self.model is None or self.tokenizer is None:\n            logger.error(\"Model not initialized. Please train or load a model first.\")\n            return\n        \n        logger.info(\"Testing model with sample questions...\")\n        self.model.eval()\n        \n        with torch.no_grad():\n            for question in test_questions:\n                input_text = f\"question: {question}\"\n                inputs = self.tokenizer(\n                    input_text, \n                    return_tensors=\"pt\", \n                    max_length=self.max_length, \n                    truncation=True\n                ).to(self.device)\n                \n                outputs = self.model.generate(\n                    **inputs, \n                    max_length=self.max_length, \n                    num_beams=5,\n                    no_repeat_ngram_size=2,\n                    early_stopping=True\n                )\n                \n                answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                print(f\"\\nQ: {question}\")\n                print(f\"A: {answer}\")\n\ndef main():\n    \"\"\"Main training pipeline\"\"\"\n    # Configuration\n    OUTPUT_DIR = \"./fitness_qa_model\"\n    TEST_SIZE = 0.2\n    RANDOM_STATE = 42\n    \n    try:\n        # Step 1: Process data\n        logger.info(\"Starting fitness Q&A model training pipeline\")\n        processor = FitnessQAProcessor()\n        df = processor.load_and_clean_data()\n        \n        if len(df) < 100:\n            logger.warning(f\"Dataset is very small ({len(df)} samples). Consider finding more data.\")\n        \n        # Step 2: Split data\n        train_df, val_df = train_test_split(\n            df, \n            test_size=TEST_SIZE, \n            random_state=RANDOM_STATE,\n            stratify=None\n        )\n        logger.info(f\"Data split - Train: {len(train_df)}, Validation: {len(val_df)}\")\n        \n        # Step 3: Save processed datasets\n        os.makedirs(\"processed_data\", exist_ok=True)\n        train_df.to_csv(\"processed_data/train_cleaned.csv\", index=False)\n        val_df.to_csv(\"processed_data/val_cleaned.csv\", index=False)\n        logger.info(\"Processed datasets saved\")\n        \n        # Step 4: Initialize trainer and train model\n        trainer = T5FitnessTrainer()\n        trained_model = trainer.train_model(train_df, val_df, OUTPUT_DIR)\n        \n        # Step 5: Test the model\n        trainer.test_model()\n        \n        logger.info(\"Pipeline completed successfully!\")\n        \n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T04:41:19.023265Z","iopub.execute_input":"2025-06-18T04:41:19.024082Z","iopub.status.idle":"2025-06-18T04:42:50.967569Z","shell.execute_reply.started":"2025-06-18T04:41:19.024056Z","shell.execute_reply":"2025-06-18T04:42:50.966256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/203 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a308addd56d42de80b7084c5e3f89f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversational_dataset.csv:   0%|          | 0.00/289k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e45446bf884370b38e508193659f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/965 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d41cc4c25af4b3c8283dac1a41970e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bbe62c827c84069b3083123b3529543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4722b555d6b64b8f9e69ccfab1d8eadd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dcbdece3e6544e18be2b732947db4d8"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='201' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [201/270 01:21 < 00:28, 2.44 it/s, Epoch 2.22/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='6' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6/12 00:02 < 00:02, 2.45 it/s]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1377601860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1377601860.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Step 4: Initialize trainer and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5FitnessTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Step 5: Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1377601860.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, val_df, output_dir)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4373\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4375\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.86 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1007.12 MiB is free. Process 3327 has 14.90 GiB memory in use. Of the allocated memory 7.67 GiB is allocated by PyTorch, and 6.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 6.86 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1007.12 MiB is free. Process 3327 has 14.90 GiB memory in use. Of the allocated memory 7.67 GiB is allocated by PyTorch, and 6.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":2}]}