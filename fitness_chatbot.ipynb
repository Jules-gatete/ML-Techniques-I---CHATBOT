{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"\"\"\"\nComplete Setup and Installation Script for Fitness Q&A Model Training\nThis script handles all installations and imports needed for the project\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef install_package(package_name, upgrade=False):\n    \"\"\"Install a package using pip\"\"\"\n    try:\n        cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n        if upgrade:\n            cmd.append(\"--upgrade\")\n        cmd.append(package_name)\n        \n        print(f\"Installing {package_name}...\")\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        print(f\"✓ Successfully installed {package_name}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"✗ Failed to install {package_name}: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\ndef check_and_install_packages():\n    \"\"\"Check and install all required packages\"\"\"\n    \n    # Core packages with specific versions for compatibility\n    # Note: PyArrow version is critical for datasets compatibility\n    packages = [\n        \"torch>=2.0.0\",\n        \"pyarrow>=12.0.0,<18.0.0\",  # Fix PyArrow compatibility issue\n        \"transformers>=4.30.0\",\n        \"datasets>=2.12.0\",\n        \"pandas>=1.5.0\",\n        \"numpy>=1.21.0\",\n        \"scikit-learn>=1.2.0\",\n        \"evaluate>=0.4.0\",\n        \"nltk>=3.8.0\",\n        \"accelerate>=0.20.0\",  # For better training performance\n        \"sentencepiece>=0.1.99\",  # Required for T5 tokenizer\n        \"protobuf>=3.20.0\",  # Required for datasets\n        \"tqdm>=4.64.0\",  # Progress bars\n        \"requests>=2.28.0\",  # For downloading datasets\n        \"huggingface_hub>=0.15.0\",  # For model/dataset downloads\n    ]\n    \n    print(\"=\" * 60)\n    print(\"INSTALLING REQUIRED PACKAGES\")\n    print(\"=\" * 60)\n    \n    failed_packages = []\n    \n    # First, upgrade pip itself\n    print(\"Upgrading pip...\")\n    try:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], \n                      check=True, capture_output=True)\n        print(\"✓ pip upgraded successfully\")\n    except:\n        print(\"⚠ Could not upgrade pip, continuing anyway...\")\n    \n    # Special handling for PyArrow compatibility issue\n    print(\"\\n🔧 Fixing PyArrow compatibility issue...\")\n    try:\n        # Uninstall potentially conflicting PyArrow versions\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], \n                      capture_output=True)\n        # Install compatible version\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow>=12.0.0,<18.0.0\"], \n                      check=True, capture_output=True)\n        print(\"✓ PyArrow compatibility fixed\")\n    except Exception as e:\n        print(f\"⚠ Could not fix PyArrow: {e}\")\n    \n    # Install packages\n    for package in packages:\n        if not install_package(package):\n            failed_packages.append(package)\n    \n    # Install optional packages for better performance\n    optional_packages = [\n        \"tensorboard\",  # For monitoring training\n        \"matplotlib\",   # For plotting if needed\n        \"seaborn\",      # For better plots\n    ]\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"INSTALLING OPTIONAL PACKAGES\")\n    print(\"=\" * 60)\n    \n    for package in optional_packages:\n        install_package(package)  # Don't track failures for optional packages\n    \n    if failed_packages:\n        print(f\"\\n⚠ Warning: Failed to install: {', '.join(failed_packages)}\")\n        print(\"You may need to install these manually or check your environment.\")\n    else:\n        print(\"\\n✓ All required packages installed successfully!\")\n    \n    return len(failed_packages) == 0\n\ndef fix_pyarrow_issue():\n    \"\"\"Fix the specific PyArrow compatibility issue\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FIXING PYARROW COMPATIBILITY ISSUE\")\n    print(\"=\" * 60)\n    \n    try:\n        # Step 1: Uninstall problematic packages\n        packages_to_remove = [\"pyarrow\", \"datasets\"]\n        for package in packages_to_remove:\n            try:\n                subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package], \n                              capture_output=True, check=False)\n                print(f\"✓ Uninstalled {package}\")\n            except:\n                print(f\"⚠ Could not uninstall {package} (may not be installed)\")\n        \n        # Step 2: Install compatible PyArrow first\n        compatible_pyarrow = \"pyarrow>=12.0.0,<15.0.0\"\n        if install_package(compatible_pyarrow):\n            print(\"✓ Installed compatible PyArrow version\")\n        else:\n            print(\"✗ Failed to install compatible PyArrow\")\n            return False\n        \n        # Step 3: Install datasets with the compatible PyArrow\n        if install_package(\"datasets>=2.12.0\"):\n            print(\"✓ Installed datasets with compatible PyArrow\")\n        else:\n            print(\"✗ Failed to install datasets\")\n            return False\n        \n        # Step 4: Test the fix\n        try:\n            import pyarrow\n            import datasets\n            print(f\"✓ PyArrow version: {pyarrow.__version__}\")\n            print(f\"✓ Datasets version: {datasets.__version__}\")\n            \n            # Test basic functionality\n            from datasets import Dataset\n            test_data = {\"text\": [\"hello\", \"world\"], \"label\": [1, 0]}\n            test_dataset = Dataset.from_dict(test_data)\n            print(\"✓ Datasets functionality test passed\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"✗ Test failed: {e}\")\n            return False\n            \n    except Exception as e:\n        print(f\"✗ PyArrow fix failed: {e}\")\n        return False\ndef verify_installations(skip_datasets=False):\n    \"\"\"Verify that all packages can be imported correctly\"\"\"\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"VERIFYING INSTALLATIONS\")\n    print(\"=\" * 60)\n    \n    imports_to_test = [\n        (\"torch\", \"PyTorch\"),\n        (\"transformers\", \"Transformers\"),\n        (\"pandas\", \"Pandas\"),\n        (\"numpy\", \"NumPy\"),\n        (\"sklearn\", \"Scikit-learn\"),\n        (\"evaluate\", \"Evaluate\"),\n        (\"nltk\", \"NLTK\"),\n        (\"accelerate\", \"Accelerate\"),\n    ]\n    \n    # Add datasets only if PyArrow was fixed\n    if not skip_datasets:\n        imports_to_test.append((\"datasets\", \"Datasets\"))\n    \n    failed_imports = []\n    \n    for module_name, display_name in imports_to_test:\n        try:\n            __import__(module_name)\n            print(f\"✓ {display_name} imported successfully\")\n        except ImportError as e:\n            print(f\"✗ Failed to import {display_name}: {e}\")\n            failed_imports.append(display_name)\n        except AttributeError as e:\n            if \"pyarrow\" in str(e).lower():\n                print(f\"✗ {display_name} failed due to PyArrow compatibility: {e}\")\n                failed_imports.append(display_name)\n            else:\n                print(f\"✗ Failed to import {display_name}: {e}\")\n                failed_imports.append(display_name)\n    \n    # Special checks\n    try:\n        import torch\n        print(f\"✓ PyTorch version: {torch.__version__}\")\n        print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            print(f\"✓ CUDA device count: {torch.cuda.device_count()}\")\n    except:\n        print(\"✗ Could not get PyTorch details\")\n    \n    try:\n        import transformers\n        print(f\"✓ Transformers version: {transformers.__version__}\")\n    except:\n        print(\"✗ Could not get Transformers version\")\n    \n    if not skip_datasets:\n        try:\n            import pyarrow\n            import datasets\n            print(f\"✓ PyArrow version: {pyarrow.__version__}\")\n            print(f\"✓ Datasets version: {datasets.__version__}\")\n        except:\n            print(\"✗ Could not get PyArrow/Datasets versions\")\n    \n    if failed_imports:\n        print(f\"\\n⚠ Warning: Failed to import: {', '.join(failed_imports)}\")\n        return False\n    else:\n        print(\"\\n✓ All packages verified successfully!\")\n        return True\n\ndef download_nltk_data():\n    \"\"\"Download required NLTK data\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"DOWNLOADING NLTK DATA\")\n    print(\"=\" * 60)\n    \n    try:\n        import nltk\n        \n        # Download required NLTK data\n        nltk_downloads = [\n            'punkt',\n            'stopwords',\n            'wordnet',\n            'omw-1.4'\n        ]\n        \n        for item in nltk_downloads:\n            try:\n                nltk.download(item, quiet=True)\n                print(f\"✓ Downloaded NLTK {item}\")\n            except Exception as e:\n                print(f\"⚠ Could not download NLTK {item}: {e}\")\n                \n    except ImportError:\n        print(\"✗ NLTK not available for downloading data\")\n\ndef setup_environment():\n    \"\"\"Set up environment variables and create directories\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SETTING UP ENVIRONMENT\")\n    print(\"=\" * 60)\n    \n    # Set environment variables\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/transformers\"\n    os.environ[\"HF_DATASETS_CACHE\"] = \"./cache/datasets\"\n    print(\"✓ Environment variables set\")\n    \n    # Create necessary directories\n    directories = [\n        \"cache\",\n        \"cache/transformers\", \n        \"cache/datasets\",\n        \"processed_data\",\n        \"models\",\n        \"logs\"\n    ]\n    \n    for directory in directories:\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        print(f\"✓ Created directory: {directory}\")\n\ndef main():\n    \"\"\"Main setup function\"\"\"\n    print(\"🚀 Starting complete setup for Fitness Q&A Model Training\")\n    print(\"This may take several minutes...\")\n    \n    # Step 0: Fix PyArrow issue first (critical fix)\n    print(\"\\n🔧 Applying critical compatibility fixes...\")\n    pyarrow_fixed = fix_pyarrow_issue()\n    \n    # Step 1: Install packages\n    packages_ok = check_and_install_packages()\n    \n    # Step 2: Verify installations (skip datasets if PyArrow fix failed)\n    imports_ok = verify_installations(skip_datasets=not pyarrow_fixed)\n    \n    # Step 3: Download NLTK data\n    download_nltk_data()\n    \n    # Step 4: Setup environment\n    setup_environment()\n    \n    # Final status\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SETUP COMPLETE\")\n    print(\"=\" * 60)\n    \n    if packages_ok and imports_ok:\n        print(\"✅ Setup completed successfully!\")\n        print(\"You can now run the fitness Q&A training script.\")\n        \n        # Test basic functionality\n        print(\"\\n🧪 Running quick functionality test...\")\n        try:\n            import torch\n            from transformers import AutoTokenizer\n            \n            # Test tokenizer loading\n            tokenizer = AutoTokenizer.from_pretrained('t5-small')\n            print(\"✓ T5 tokenizer loads correctly\")\n            \n            # Test basic tokenization\n            test_text = \"question: How can I improve my fitness?\"\n            tokens = tokenizer(test_text, return_tensors=\"pt\")\n            print(\"✓ Tokenization works correctly\")\n            \n            # Test datasets if PyArrow was fixed\n            if pyarrow_fixed:\n                from datasets import Dataset\n                test_data = {\"question\": [\"test?\"], \"answer\": [\"test answer\"]}\n                test_ds = Dataset.from_dict(test_data)\n                print(\"✓ Datasets functionality works correctly\")\n            \n            print(\"✅ All functionality tests passed!\")\n            \n        except Exception as e:\n            print(f\"⚠ Functionality test failed: {e}\")\n            print(\"You may need to restart your Python session.\")\n    \n    else:\n        print(\"❌ Setup completed with some issues.\")\n        print(\"Please check the error messages above and resolve any missing packages.\")\n        if not pyarrow_fixed:\n            print(\"\\n🔧 PyArrow compatibility issue detected!\")\n            print(\"Try running this fix manually:\")\n            print(\"pip uninstall -y pyarrow datasets\")\n            print(\"pip install 'pyarrow>=12.0.0,<15.0.0'\")\n            print(\"pip install 'datasets>=2.12.0'\")\n    \n    print(\"\\n📋 Next steps:\")\n    print(\"1. If setup was successful, you can now run the main training script\")\n    print(\"2. If there were issues, please install missing packages manually\")\n    print(\"3. Consider restarting your Python kernel/session after installation\")\n    print(\"4. If PyArrow issues persist, try the manual fix commands above\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n# =============================================================================\n# IMPORTS SECTION - All imports needed for the main script\n# =============================================================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTING ALL IMPORTS FOR MAIN SCRIPT\")\nprint(\"=\" * 60)\n\ntry:\n    # Standard library imports\n    import os\n    import re\n    import logging\n    from pathlib import Path\n    from typing import Dict, List, Optional, Tuple\n    print(\"✓ Standard library imports successful\")\n    \n    # Data handling\n    import pandas as pd\n    import numpy as np\n    print(\"✓ Data handling imports successful\")\n    \n    # Machine learning\n    from sklearn.model_selection import train_test_split\n    print(\"✓ Scikit-learn imports successful\")\n    \n    # PyTorch\n    import torch\n    print(\"✓ PyTorch import successful\")\n    \n    # NLTK\n    import nltk\n    print(\"✓ NLTK import successful\")\n    \n    # Hugging Face\n    from datasets import Dataset, load_dataset\n    from transformers import (\n        AutoTokenizer, \n        T5ForConditionalGeneration, \n        Trainer, \n        TrainingArguments,\n        EarlyStoppingCallback\n    )\n    print(\"✓ Transformers imports successful\")\n    \n    # Evaluation\n    import evaluate\n    print(\"✓ Evaluate import successful\")\n    \n    print(\"\\n✅ ALL IMPORTS SUCCESSFUL - Ready to run main script!\")\n    \nexcept ImportError as e:\n    print(f\"\\n❌ Import failed: {e}\")\n    print(\"Please run the setup section above to install missing packages.\")\n\n# =============================================================================\n# SYSTEM INFO\n# =============================================================================\n\ndef print_system_info():\n    \"\"\"Print system information for debugging\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SYSTEM INFORMATION\")\n    print(\"=\" * 60)\n    \n    print(f\"Python version: {sys.version}\")\n    print(f\"Platform: {sys.platform}\")\n    \n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    except:\n        pass\n    \n    try:\n        import transformers\n        print(f\"Transformers version: {transformers.__version__}\")\n    except:\n        pass\n    \n    try:\n        import datasets\n        print(f\"Datasets version: {datasets.__version__}\")\n    except:\n        pass\n\nprint_system_info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:25:48.602141Z","iopub.execute_input":"2025-06-19T11:25:48.602400Z","iopub.status.idle":"2025-06-19T11:28:29.118841Z","shell.execute_reply.started":"2025-06-19T11:25:48.602379Z","shell.execute_reply":"2025-06-19T11:28:29.118017Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting complete setup for Fitness Q&A Model Training\nThis may take several minutes...\n\n🔧 Applying critical compatibility fixes...\n\n============================================================\nFIXING PYARROW COMPATIBILITY ISSUE\n============================================================\n✓ Uninstalled pyarrow\n✓ Uninstalled datasets\nInstalling pyarrow>=12.0.0,<15.0.0...\n✓ Successfully installed pyarrow>=12.0.0,<15.0.0\n✓ Installed compatible PyArrow version\nInstalling datasets>=2.12.0...\n✓ Successfully installed datasets>=2.12.0\n✓ Installed datasets with compatible PyArrow\n✓ PyArrow version: 20.0.0\n✓ Datasets version: 3.6.0\n✓ Datasets functionality test passed\n============================================================\nINSTALLING REQUIRED PACKAGES\n============================================================\nUpgrading pip...\n✓ pip upgraded successfully\n\n🔧 Fixing PyArrow compatibility issue...\n✓ PyArrow compatibility fixed\nInstalling torch>=2.0.0...\n✓ Successfully installed torch>=2.0.0\nInstalling pyarrow>=12.0.0,<18.0.0...\n✓ Successfully installed pyarrow>=12.0.0,<18.0.0\nInstalling transformers>=4.30.0...\n✓ Successfully installed transformers>=4.30.0\nInstalling datasets>=2.12.0...\n✓ Successfully installed datasets>=2.12.0\nInstalling pandas>=1.5.0...\n✓ Successfully installed pandas>=1.5.0\nInstalling numpy>=1.21.0...\n✓ Successfully installed numpy>=1.21.0\nInstalling scikit-learn>=1.2.0...\n✓ Successfully installed scikit-learn>=1.2.0\nInstalling evaluate>=0.4.0...\n✓ Successfully installed evaluate>=0.4.0\nInstalling nltk>=3.8.0...\n✓ Successfully installed nltk>=3.8.0\nInstalling accelerate>=0.20.0...\n✓ Successfully installed accelerate>=0.20.0\nInstalling sentencepiece>=0.1.99...\n✓ Successfully installed sentencepiece>=0.1.99\nInstalling protobuf>=3.20.0...\n✓ Successfully installed protobuf>=3.20.0\nInstalling tqdm>=4.64.0...\n✓ Successfully installed tqdm>=4.64.0\nInstalling requests>=2.28.0...\n✓ Successfully installed requests>=2.28.0\nInstalling huggingface_hub>=0.15.0...\n✓ Successfully installed huggingface_hub>=0.15.0\n\n============================================================\nINSTALLING OPTIONAL PACKAGES\n============================================================\nInstalling tensorboard...\n✓ Successfully installed tensorboard\nInstalling matplotlib...\n✓ Successfully installed matplotlib\nInstalling seaborn...\n✓ Successfully installed seaborn\n\n✓ All required packages installed successfully!\n\n============================================================\nVERIFYING INSTALLATIONS\n============================================================\n✓ PyTorch imported successfully\n✓ Transformers imported successfully\n✓ Pandas imported successfully\n✓ NumPy imported successfully\n✓ Scikit-learn imported successfully\n","output_type":"stream"},{"name":"stderr","text":"2025-06-19 11:28:07.023343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750332487.231120      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750332487.287037      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ Evaluate imported successfully\n✓ NLTK imported successfully\n✓ Accelerate imported successfully\n✓ Datasets imported successfully\n✓ PyTorch version: 2.6.0+cu124\n✓ CUDA available: True\n✓ CUDA device count: 1\n✓ Transformers version: 4.51.3\n✓ PyArrow version: 20.0.0\n✓ Datasets version: 3.6.0\n\n✓ All packages verified successfully!\n\n============================================================\nDOWNLOADING NLTK DATA\n============================================================\n✓ Downloaded NLTK punkt\n✓ Downloaded NLTK stopwords\n✓ Downloaded NLTK wordnet\n✓ Downloaded NLTK omw-1.4\n\n============================================================\nSETTING UP ENVIRONMENT\n============================================================\n✓ Environment variables set\n✓ Created directory: cache\n✓ Created directory: cache/transformers\n✓ Created directory: cache/datasets\n✓ Created directory: processed_data\n✓ Created directory: models\n✓ Created directory: logs\n\n============================================================\nSETUP COMPLETE\n============================================================\n✅ Setup completed successfully!\nYou can now run the fitness Q&A training script.\n\n🧪 Running quick functionality test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41c97e6b8164e01a8f38620145435a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0157d8f428224302a1013e1951b268ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d648a8a90e25440d8d3ed4d37a458dd7"}},"metadata":{}},{"name":"stdout","text":"✓ T5 tokenizer loads correctly\n✓ Tokenization works correctly\n✓ Datasets functionality works correctly\n✅ All functionality tests passed!\n\n📋 Next steps:\n1. If setup was successful, you can now run the main training script\n2. If there were issues, please install missing packages manually\n3. Consider restarting your Python kernel/session after installation\n4. If PyArrow issues persist, try the manual fix commands above\n\n============================================================\nTESTING ALL IMPORTS FOR MAIN SCRIPT\n============================================================\n✓ Standard library imports successful\n✓ Data handling imports successful\n✓ Scikit-learn imports successful\n✓ PyTorch import successful\n✓ NLTK import successful\n✓ Transformers imports successful\n✓ Evaluate import successful\n\n✅ ALL IMPORTS SUCCESSFUL - Ready to run main script!\n\n============================================================\nSYSTEM INFORMATION\n============================================================\nPython version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nPlatform: linux\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nGPU count: 1\nGPU 0: Tesla P100-PCIE-16GB\nTransformers version: 4.51.3\nDatasets version: 3.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 1: Install necessary libraries (run in terminal or notebook if needed)\n# !pip install datasets transformers sentence-transformers pandas scikit-learn numpy\n\n# Step 2: Import required libraries\nimport pandas as pd\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Step 3: Define utility functions\n\ndef clean_text(text):\n    \"\"\"\n    Clean text by converting to lowercase, removing extra spaces, and special characters.\n    Args:\n        text (str): Input text to clean.\n    Returns:\n        str: Cleaned text.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Remove special characters except punctuation\n    return text\n\ndef filter_fitness_relevance(df, question_col, keywords):\n    \"\"\"\n    Filter dataset to keep only fitness-related questions based on keywords.\n    Args:\n        df (pd.DataFrame): Input dataframe with question column.\n        question_col (str): Name of the question column.\n        keywords (list): List of fitness-related keywords.\n    Returns:\n        pd.DataFrame: Filtered dataframe.\n    \"\"\"\n    pattern = '|'.join(keywords)\n    return df[df[question_col].str.contains(pattern, case=False, na=False)]\n\ndef paraphrase_question(question, paraphraser):\n    \"\"\"\n    Generate a paraphrased version of the input question using a sentence transformer.\n    Args:\n        question (str): Original question.\n        paraphraser: SentenceTransformer model for paraphrasing.\n    Returns:\n        str: Paraphrased question (placeholder, can be enhanced).\n    \"\"\"\n    # Placeholder: In practice, use paraphraser.encode() to generate embeddings and find similar phrasing\n    return question  # Replace with actual paraphrasing logic if needed\n\ndef augment_data(df, question_col, answer_col, paraphraser, num_augmentations=1):\n    \"\"\"\n    Augment dataset by generating paraphrased questions.\n    Args:\n        df (pd.DataFrame): Input dataframe with question and answer columns.\n        question_col (str): Name of the question column.\n        answer_col (str): Name of the answer column.\n        paraphraser: SentenceTransformer model for paraphrasing.\n        num_augmentations (int): Number of paraphrases per question.\n    Returns:\n        pd.DataFrame: Augmented dataframe.\n    \"\"\"\n    augmented_rows = []\n    for _, row in df.iterrows():\n        original_question = row[question_col]\n        answer = row[answer_col]\n        augmented_rows.append({question_col: original_question, answer_col: answer})\n        for _ in range(num_augmentations):\n            paraphrased_question = paraphrase_question(original_question, paraphraser)\n            augmented_rows.append({question_col: paraphrased_question, answer_col: answer})\n    return pd.DataFrame(augmented_rows)\n\ndef tokenize_data(row, question_col, answer_col, tokenizer, max_length=512):\n    \"\"\"\n    Tokenize question-answer pair for T5 model.\n    Args:\n        row (pd.Series): Dataframe row with question and answer.\n        question_col (str): Name of the question column.\n        answer_col (str): Name of the answer column.\n        tokenizer: Transformers tokenizer.\n        max_length (int): Maximum token length.\n    Returns:\n        dict: Tokenized input and attention mask.\n    \"\"\"\n    input_text = f\"question: {row[question_col]} answer: {row[answer_col]}\"\n    return tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n\ndef main():\n    # Step 4: Load the dataset from Hugging Face\n    try:\n        dataset = load_dataset(\"its-myrto/fitness-question-answers\")\n        df = dataset['train'].to_pandas()\n        print(f\"Initial dataset size: {len(df)}\")\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return\n\n    # Step 5: Inspect column names\n    print(\"Dataset columns:\", df.columns.tolist())\n\n    # Step 6: Define column names (update these based on actual column names)\n    # Based on error, 'question' and 'answer' may not exist. Adjust as needed.\n    # For now, let's assume they might be 'Question' and 'Answer' (case-sensitive) or check output above.\n    question_col = 'Question'  # Update this after checking printed columns\n    answer_col = 'Answer'      # Update this after checking printed columns\n\n    # Verify column names exist\n    if question_col not in df.columns or answer_col not in df.columns:\n        print(f\"Error: Columns '{question_col}' and/or '{answer_col}' not found in dataset.\")\n        print(\"Please update 'question_col' and 'answer_col' in the script with correct column names.\")\n        return\n\n    # Step 7: Clean the data\n    # Remove duplicates\n    df = df.drop_duplicates(subset=[question_col, answer_col], keep='first')\n    print(f\"Rows after removing duplicates: {len(df)}\")\n\n    # Remove missing values\n    df = df.dropna(subset=[question_col, answer_col])\n    print(f\"Rows after removing missing values: {len(df)}\")\n\n    # Clean questions and answers\n    df[question_col] = df[question_col].apply(clean_text)\n    df[answer_col] = df[answer_col].apply(clean_text)\n\n    # Step 8: Filter for fitness relevance\n    fitness_keywords = ['exercise', 'workout', 'fitness', 'nutrition', 'muscle', 'cardio', 'strength', 'yoga', 'running']\n    df = filter_fitness_relevance(df, question_col, fitness_keywords)\n    print(f\"Rows after filtering for fitness relevance: {len(df)}\")\n\n    # Step 9: Data augmentation (optional, enabled if dataset is small)\n    if len(df) < 1000:\n        print(\"Augmenting dataset due to small size...\")\n        try:\n            paraphraser = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n            df = augment_data(df, question_col, answer_col, paraphraser, num_augmentations=1)\n            print(f\"Rows after augmentation: {len(df)}\")\n        except Exception as e:\n            print(f\"Augmentation failed: {e}. Proceeding without augmentation.\")\n            # Continue without augmentation if it fails\n\n    # Step 10: Split dataset into train and validation\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n\n    # Step 11: Tokenize data for T5 model\n    try:\n        tokenizer = AutoTokenizer.from_pretrained('t5-small')\n        train_tokens = train_df.apply(lambda row: tokenize_data(row, question_col, answer_col, tokenizer), axis=1)\n        val_tokens = val_df.apply(lambda row: tokenize_data(row, question_col, answer_col, tokenizer), axis=1)\n    except Exception as e:\n        print(f\"Tokenization failed: {e}\")\n        return\n\n      # Step 12: Save cleaned and split datasets\n    output_dir = \"/kaggle/working/processed_data\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    train_df.to_csv(os.path.join(output_dir, 'train_cleaned.csv'), index=False)\n    val_df.to_csv(os.path.join(output_dir, 'val_cleaned.csv'), index=False)\n    print(\"Cleaned datasets saved in '/kaggle/working/processed_data/'\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:28:29.120291Z","iopub.execute_input":"2025-06-19T11:28:29.120545Z","iopub.status.idle":"2025-06-19T11:28:47.166732Z","shell.execute_reply.started":"2025-06-19T11:28:29.120528Z","shell.execute_reply":"2025-06-19T11:28:47.165880Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/203 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5833e98d3785453b9837c5b2eb21ac84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversational_dataset.csv:   0%|          | 0.00/289k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5342265361ce42b2b1a9db7a26c5fbae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/965 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05e49656b074e6a94cf75118775836e"}},"metadata":{}},{"name":"stdout","text":"Initial dataset size: 965\nDataset columns: ['Unnamed: 0', 'Question', 'Answer']\nRows after removing duplicates: 965\nRows after removing missing values: 965\nRows after filtering for fitness relevance: 515\nAugmenting dataset due to small size...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76108228266e4be5852c1bad329578fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429dce6768884ea08e1dc30595488e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c7dc4df0494f149ac9757325b9cc80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c07b88095d448c6ac6f325f792ced07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cf33c2694c4ac0bc08014356bd76cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd63417b035541099d93ede58a38ceae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539b5ea0bee045f4be163de83e2e1f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664a069cfbd948639ecc01e52d53d0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d909a45ca5456cb2d556864358c565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c615f0f60c43b2bbd8c5b8ce515cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b7183e79964b4792eeaa6824ce6f9b"}},"metadata":{}},{"name":"stdout","text":"Rows after augmentation: 1030\nTrain size: 824, Validation size: 206\nCleaned datasets saved in '/kaggle/working/processed_data/'\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport re\n\ndef clean_invalid_questions():\n    files = ['train_cleaned.csv', 'val_cleaned.csv']\n    for file in files:\n        try:\n            df = pd.read_csv(f\"/kaggle/working/processed_data/{file}\")\n            print(f\"\\nProcessing {file}: {len(df)} rows\")\n            # Remove rows where 'Question' is not a valid question\n            df = df[df['Question'].str.contains(r'[a-zA-Z\\s]+[?]', na=False)]\n            # Remove rows with non-fitness-related or invalid terms\n            invalid_terms = ['entailment', 'true', 'false', 'contradiction']\n            df = df[~df['Question'].str.lower().str.contains('|'.join(invalid_terms), na=False)]\n            print(f\"After removing invalid questions: {len(df)} rows\")\n            # Save cleaned file\n            df.to_csv(f\"/kaggle/working/processed_data/{file}\", index=False)\n            print(f\"Saved cleaned {file}\")\n            # Preview cleaned data\n            print(f\"Preview of cleaned {file}:\")\n            print(df.head(5))\n        except Exception as e:\n            print(f\"Error processing {file}: {e}\")\n\nif __name__ == \"__main__\":\n    clean_invalid_questions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:28:47.167691Z","iopub.execute_input":"2025-06-19T11:28:47.167920Z","iopub.status.idle":"2025-06-19T11:28:47.202832Z","shell.execute_reply.started":"2025-06-19T11:28:47.167903Z","shell.execute_reply":"2025-06-19T11:28:47.202163Z"}},"outputs":[{"name":"stdout","text":"\nProcessing train_cleaned.csv: 824 rows\nAfter removing invalid questions: 824 rows\nSaved cleaned train_cleaned.csv\nPreview of cleaned train_cleaned.csv:\n                                            Question  \\\n0    how does strength training improve flexibility?   \n1  how do i know if ive worked a muscle hard enough?   \n2  what are some simple exercises i can do at hom...   \n3                         how to strengthen my knee?   \n4  is it okay to skip a workout if im feeling tired?   \n\n                                              Answer  \n0  strength training can improve flexibility by i...  \n1  if you feel a burn in the muscle during the la...  \n2  there are plenty of exercises you can do at ho...  \n3  exercise is a noninvasive and healthful way to...  \n4  its okay to skip a workout if youre feeling ti...  \n\nProcessing val_cleaned.csv: 206 rows\nAfter removing invalid questions: 206 rows\nSaved cleaned val_cleaned.csv\nPreview of cleaned val_cleaned.csv:\n                                            Question  \\\n0  how can i develop a consistent exercise routin...   \n1     what are some workout suggestions for adults ?   \n2  what are some nonrunning exercise options that...   \n3  i have type 2 diabetes. what types of exercise...   \n4                    who are liss workouts best for?   \n\n                                              Answer  \n0  set realistic and achievable fitness goals to ...  \n1  one example is to do moderateintensity aerobic...  \n2  some nonrunning exercise options include using...  \n3  for individuals with type 2 diabetes, engaging...  \n4  liss workouts are ideal for those looking to i...  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\n\ndef clean_invalid_questions():\n    files = ['train_cleaned.csv', 'val_cleaned.csv']\n    for file in files:\n        try:\n            df = pd.read_csv(f\"/kaggle/working/processed_data/{file}\")\n            print(f\"\\nProcessing {file}: {len(df)} rows\")\n            # Remove rows where 'Question' is not a valid question\n            df = df[df['Question'].str.contains(r'[a-zA-Z\\s]+[?]', na=False)]\n            # Remove rows with non-fitness-related or invalid terms\n            invalid_terms = ['entailment', 'true', 'false', 'contradiction']\n            df = df[~df['Question'].str.lower().str.contains('|'.join(invalid_terms), na=False)]\n            print(f\"After removing invalid questions: {len(df)} rows\")\n            # Save cleaned file\n            df.to_csv(f\"/kaggle/working/processed_data/{file}\", index=False)\n            print(f\"Saved cleaned {file}\")\n            # Preview cleaned data\n            print(f\"Preview of cleaned {file}:\")\n            print(df.head(5))\n        except Exception as e:\n            print(f\"Error processing {file}: {e}\")\n\nif __name__ == \"__main__\":\n    clean_invalid_questions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:28:47.203487Z","iopub.execute_input":"2025-06-19T11:28:47.203751Z","iopub.status.idle":"2025-06-19T11:28:47.263894Z","shell.execute_reply.started":"2025-06-19T11:28:47.203721Z","shell.execute_reply":"2025-06-19T11:28:47.263329Z"}},"outputs":[{"name":"stdout","text":"\nProcessing train_cleaned.csv: 824 rows\nAfter removing invalid questions: 824 rows\nSaved cleaned train_cleaned.csv\nPreview of cleaned train_cleaned.csv:\n                                            Question  \\\n0    how does strength training improve flexibility?   \n1  how do i know if ive worked a muscle hard enough?   \n2  what are some simple exercises i can do at hom...   \n3                         how to strengthen my knee?   \n4  is it okay to skip a workout if im feeling tired?   \n\n                                              Answer  \n0  strength training can improve flexibility by i...  \n1  if you feel a burn in the muscle during the la...  \n2  there are plenty of exercises you can do at ho...  \n3  exercise is a noninvasive and healthful way to...  \n4  its okay to skip a workout if youre feeling ti...  \n\nProcessing val_cleaned.csv: 206 rows\nAfter removing invalid questions: 206 rows\nSaved cleaned val_cleaned.csv\nPreview of cleaned val_cleaned.csv:\n                                            Question  \\\n0  how can i develop a consistent exercise routin...   \n1     what are some workout suggestions for adults ?   \n2  what are some nonrunning exercise options that...   \n3  i have type 2 diabetes. what types of exercise...   \n4                    who are liss workouts best for?   \n\n                                              Answer  \n0  set realistic and achievable fitness goals to ...  \n1  one example is to do moderateintensity aerobic...  \n2  some nonrunning exercise options include using...  \n3  for individuals with type 2 diabetes, engaging...  \n4  liss workouts are ideal for those looking to i...  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nimport os\nimport re\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport torch\nimport nltk\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    T5ForConditionalGeneration,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\nfrom sklearn.model_selection import train_test_split\nimport evaluate\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Memory management\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch.cuda.empty_cache()\n\ndef format_prompt(question: str, prompt_type: str = \"instruct\") -> str:\n    if prompt_type == \"instruct\":\n        return f\"Answer the following fitness question: {question}\"\n    elif prompt_type == \"qa\":\n        return f\"Q: {question}\\nA:\"\n    else:\n        return f\"question: {question}\"\n\nclass FitnessQAProcessor:\n    FITNESS_KEYWORDS = [\n        'exercise', 'workout', 'fitness', 'nutrition', 'muscle', 'cardio',\n        'strength', 'yoga', 'running', 'sleep', 'stress', 'recovery',\n        'flexibility', 'balance', 'posture', 'hydration', 'motivation',\n        'diet', 'weight', 'training', 'gym', 'health', 'stretch'\n    ]\n\n    def __init__(self):\n        self.question_col = 'Question'\n        self.answer_col = 'Answer'\n\n    def clean_text(self, text: str) -> str:\n        if not isinstance(text, str) or pd.isna(text):\n            return \"\"\n        text = text.lower().strip()\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n        if len(text.split()) < 3:\n            return \"\"\n        return text\n\n    def is_valid_question(self, question: str) -> bool:\n        if not question or len(question) < 10:\n            return False\n        if not re.search(r'[a-zA-Z]', question):\n            return False\n        question_indicators = ['how', 'what', 'why', 'when', 'where', 'which', 'who', 'can', 'should', 'do', 'does', 'is', 'are']\n        return question.endswith('?') or any(question.startswith(word) for word in question_indicators)\n\n    def is_fitness_related(self, text: str) -> bool:\n        pattern = '|'.join(self.FITNESS_KEYWORDS)\n        return bool(re.search(pattern, text, re.IGNORECASE))\n\n    def load_and_clean_data(self, dataset_name: str = \"its-myrto/fitness-question-answers\") -> pd.DataFrame:\n        logger.info(f\"Loading dataset: {dataset_name}\")\n        dataset = load_dataset(dataset_name)\n        df = dataset['train'].to_pandas()\n        if 'Unnamed: 0' in df.columns:\n            df.drop(columns=['Unnamed: 0'], inplace=True)\n        if self.question_col not in df.columns or self.answer_col not in df.columns:\n            raise ValueError(f\"Required columns '{self.question_col}' and/or '{self.answer_col}' not found\")\n        return self._clean_dataframe(df)\n\n    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = df.drop_duplicates(subset=[self.question_col, self.answer_col])\n        df = df.dropna(subset=[self.question_col, self.answer_col])\n        df[self.question_col] = df[self.question_col].apply(self.clean_text)\n        df[self.answer_col] = df[self.answer_col].apply(self.clean_text)\n        df = df[(df[self.question_col] != \"\") & (df[self.answer_col] != \"\")]\n        df = df[df[self.question_col].apply(self.is_valid_question)]\n        fitness_mask = (df[self.question_col].apply(self.is_fitness_related) | df[self.answer_col].apply(self.is_fitness_related))\n        return df[fitness_mask].reset_index(drop=True)\n\nclass T5FitnessTrainer:\n    def __init__(self, model_name: str = 't5-small', max_length: int = 512):\n        self.model_name = model_name\n        self.max_length = max_length\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        nltk.download('punkt', quiet=True)\n        self.tokenizer = None\n        self.model = None\n\n    def initialize_model(self):\n        logger.info(f\"Initializing {self.model_name} on {self.device}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n        self.model.gradient_checkpointing_enable()\n        self.model.to(self.device)\n\n    def prepare_dataset(self, df: pd.DataFrame) -> Dataset:\n        inputs = [format_prompt(row['Question']) for _, row in df.iterrows()]\n        targets = [row['Answer'] for _, row in df.iterrows()]\n        input_encodings = self.tokenizer(inputs, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        target_encodings = self.tokenizer(targets, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        dataset_dict = {\n            'input_ids': input_encodings['input_ids'].squeeze(),\n            'attention_mask': input_encodings['attention_mask'].squeeze(),\n            'labels': target_encodings['input_ids'].squeeze()\n        }\n        return Dataset.from_dict(dataset_dict)\n\n    def train_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame, output_dir: str) -> Trainer:\n        if self.model is None or self.tokenizer is None:\n            self.initialize_model()\n        train_dataset = self.prepare_dataset(train_df)\n        val_dataset = self.prepare_dataset(val_df)\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=15,\n            per_device_train_batch_size=1,\n            per_device_eval_batch_size=2,\n            gradient_accumulation_steps=4,\n            gradient_checkpointing=True,\n            warmup_steps=200,\n            weight_decay=0.01,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=50,\n            eval_strategy=\"steps\",\n            eval_steps=200,\n            save_strategy=\"steps\",\n            save_steps=200,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            greater_is_better=False,\n            save_total_limit=2,\n            fp16=torch.cuda.is_available(),\n            dataloader_pin_memory=False,\n            report_to=[],\n            batch_eval_metrics=False\n        )\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            compute_metrics=None,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n        trainer.save_model(f\"{output_dir}_final\")\n        self.tokenizer.save_pretrained(f\"{output_dir}_final\")\n        return trainer\n\n    def test_model(self, test_questions: Optional[List[str]] = None, prompt_type: str = \"instruct\") -> None:\n        if self.model is None or self.tokenizer is None:\n            model_path = \"/kaggle/working/fitness_qa_model_final\"\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n            self.model = T5ForConditionalGeneration.from_pretrained(model_path).to(self.device)\n        if test_questions is None:\n            test_questions = [\n                \"How can I improve my running endurance?\",\n                \"What are effective core exercises?\",\n                \"How do I stay motivated for workouts?\",\n                \"What should I eat before exercising?\",\n                \"How often should I rest between workouts?\"\n            ]\n        self.model.eval()\n        with torch.no_grad():\n            for question in test_questions:\n                prompt = format_prompt(question, prompt_type)\n                inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True).to(self.device)\n                outputs = self.model.generate(**inputs, max_length=self.max_length, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n                print(f\"\\nQ: {question}\\nA: {self.tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n\n    def evaluate_bleu(self, val_df: pd.DataFrame, sample_size: int = 100, prompt_type: str = \"instruct\") -> float:\n        bleu = evaluate.load(\"bleu\")\n        preds, refs = [], []\n        self.model.eval()\n        with torch.no_grad():\n            for i, row in enumerate(val_df.itertuples()):\n                if i >= sample_size: break\n                prompt = format_prompt(row.Question, prompt_type)\n                inputs = self.tokenizer(prompt, return_tensors='pt', max_length=self.max_length, truncation=True).to(self.device)\n                outputs = self.model.generate(**inputs, max_length=64, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n                preds.append(self.tokenizer.decode(outputs[0], skip_special_tokens=True))\n                refs.append(row.Answer)\n        score = bleu.compute(predictions=preds, references=[[r] for r in refs])[\"bleu\"]\n        logger.info(f\"BLEU score: {score:.4f}\")\n        return score\n\ndef main():\n    OUTPUT_DIR = \"/kaggle/working/processed_data/fitness_qa_model\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    os.makedirs(f\"{OUTPUT_DIR}/logs\", exist_ok=True)\n    os.makedirs(f\"{OUTPUT_DIR}_final\", exist_ok=True)\n    processor = FitnessQAProcessor()\n    df = processor.load_and_clean_data()\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n    train_df.to_csv(\"/kaggle/working/processed_data/train_cleaned.csv\", index=False)\n    val_df.to_csv(\"/kaggle/working/processed_data/val_cleaned.csv\", index=False)\n    trainer = T5FitnessTrainer()\n    trainer.train_model(train_df, val_df, output_dir=OUTPUT_DIR)\n    trainer.test_model(prompt_type=\"instruct\")\n    trainer.evaluate_bleu(val_df, sample_size=100, prompt_type=\"instruct\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:28:47.265734Z","iopub.execute_input":"2025-06-19T11:28:47.265924Z","iopub.status.idle":"2025-06-19T11:47:56.406073Z","shell.execute_reply.started":"2025-06-19T11:28:47.265908Z","shell.execute_reply":"2025-06-19T11:47:56.405478Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f6e44df5f14360868c8931691caec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c818eebae1b1411a8df3dcf3fc67553c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b93f62fc7eb4ed19fc21e0901d33205"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2670' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2670/2670 18:03, Epoch 14/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.520900</td>\n      <td>0.402534</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.371300</td>\n      <td>0.341175</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.358600</td>\n      <td>0.319831</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.338200</td>\n      <td>0.314006</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.336100</td>\n      <td>0.311116</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.364400</td>\n      <td>0.308658</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.332000</td>\n      <td>0.306921</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.320900</td>\n      <td>0.305729</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.315200</td>\n      <td>0.304760</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.314200</td>\n      <td>0.304101</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.320700</td>\n      <td>0.303594</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.308600</td>\n      <td>0.303227</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.310300</td>\n      <td>0.303037</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"\nQ: How can I improve my running endurance?\nA: running endurance is a great way to improve your running performance. if you are looking for strength training, it can help.\n\nQ: What are effective core exercises?\nA: effective core exercises include squats, tai chi, and hamstrings.\n\nQ: How do I stay motivated for workouts?\nA: staying motivated for workouts is a great way to stay motivated.\n\nQ: What should I eat before exercising?\nA: eat a lot of food before exercising, especially if you are exercising.\n\nQ: How often should I rest between workouts?\nA: rest between workouts depends on the intensity of the workout.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa94d0049ac41eab43d5178690fb068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09197b2549c4d93966def6d369ca18e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c3e536544a4f3884f28ba49644fbc9"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, T5ForConditionalGeneration\nimport shutil\nimport torch\n\nmodel_path = \"/kaggle/working/processed_data/fitness_qa_model_final\"\nbest_model_path = \"/kaggle/working/processed_data/fitness_qa_model_best_model_1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Save under new name\ntokenizer.save_pretrained(best_model_path)\nmodel.save_pretrained(best_model_path)\n\nprint(f\"Model saved as: {best_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:47:56.407863Z","iopub.execute_input":"2025-06-19T11:47:56.408062Z","iopub.status.idle":"2025-06-19T11:47:57.168232Z","shell.execute_reply.started":"2025-06-19T11:47:56.408047Z","shell.execute_reply":"2025-06-19T11:47:57.167242Z"}},"outputs":[{"name":"stdout","text":"Model saved as: /kaggle/working/processed_data/fitness_qa_model_best_model_1\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 📊 Evaluation Cell: BLEU, F1, Perplexity\n\nfrom math import exp\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport evaluate\nfrom tqdm import tqdm\nimport pandas as pd\nimport re, string\n\n# 📌 Load validation set if not already loaded\ntry:\n    val_df\nexcept NameError:\n    val_df = pd.read_csv(\"processed_data/val_cleaned.csv\")\n    print(f\"✅ Loaded val_df with {len(val_df)} samples\")\n\n# 🔧 Text normalization\ndef normalize_answer(s):\n    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text): return ' '.join(text.split())\n    def remove_punc(text): return ''.join(ch for ch in text if ch not in set(string.punctuation))\n    def lower(text): return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n# 🔧 F1 computation\ndef compute_f1(pred, true):\n    pred_tokens = normalize_answer(pred).split()\n    true_tokens = normalize_answer(true).split()\n    common = set(pred_tokens) & set(true_tokens)\n    if len(common) == 0: return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(true_tokens)\n    return 2 * (precision * recall) / (precision + recall)\n\n# 🔧 Prompt formatting\ndef format_prompt(question, prompt_type=\"instruct\"):\n    if prompt_type == \"instruct\":\n        return f\"Answer this fitness question: {question}\"\n    elif prompt_type == \"qa\":\n        return f\"question: {question}\"\n    else:\n        return question\n\n# 🧪 Evaluation function\ndef evaluate_model(model, tokenizer, val_df, device='cuda', max_length=512, sample_size=100, prompt_type=\"instruct\"):\n    bleu = evaluate.load(\"bleu\")\n    preds, refs, f1s, perplexities = [], [], [], []\n\n    model.eval()\n    with torch.no_grad():\n        for i, row in tqdm(enumerate(val_df.itertuples()), total=min(sample_size, len(val_df))):\n            if i >= sample_size: break\n\n            prompt = format_prompt(row.Question, prompt_type)\n            inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n            labels = tokenizer(row.Answer, return_tensors='pt', truncation=True, max_length=max_length).input_ids.to(device)\n\n            # Generate prediction\n            outputs = model.generate(**inputs, max_length=64)\n            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            preds.append(decoded)\n            refs.append([row.Answer])\n            f1s.append(compute_f1(decoded, row.Answer))\n\n            # Compute perplexity\n            logits = model(**inputs, labels=labels).logits\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)),\n                                   shift_labels.view(-1), ignore_index=tokenizer.pad_token_id)\n            perplexities.append(exp(loss.item()))\n\n    # 📈 Final metrics\n    bleu_score = bleu.compute(predictions=preds, references=refs)[\"bleu\"]\n    f1_score_avg = np.mean(f1s)\n    perplexity_avg = np.mean(perplexities)\n\n    print(f\"🔹 BLEU Score     : {bleu_score:.4f}\")\n    print(f\"🔹 F1 Score       : {f1_score_avg*100:.2f}%\")\n    print(f\"🔹 Perplexity     : {perplexity_avg:.2f}\")\n\n# ✅ Example call:\n# evaluate_model(model, tokenizer, val_df, device='cuda')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:47:57.169130Z","iopub.execute_input":"2025-06-19T11:47:57.169482Z","iopub.status.idle":"2025-06-19T11:47:57.190041Z","shell.execute_reply.started":"2025-06-19T11:47:57.169457Z","shell.execute_reply":"2025-06-19T11:47:57.189375Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded val_df with 179 samples\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"evaluate_model(model, tokenizer, val_df, device='cuda')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:47:57.190931Z","iopub.execute_input":"2025-06-19T11:47:57.191200Z","iopub.status.idle":"2025-06-19T11:48:30.669782Z","shell.execute_reply.started":"2025-06-19T11:47:57.191181Z","shell.execute_reply":"2025-06-19T11:48:30.669037Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:31<00:00,  3.14it/s]","output_type":"stream"},{"name":"stdout","text":"🔹 BLEU Score     : 0.0361\n🔹 F1 Score       : 17.03%\n🔹 Perplexity     : 10819.09\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport logging\nimport re\nimport sys\nimport os\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\nfrom typing import List, Optional\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass FitnessChatbot:\n    # Define fitness-related keywords\n    FITNESS_KEYWORDS = [\n        'exercise', 'workout', 'fitness', 'nutrition', 'muscle', 'cardio', 'strength', \n        'yoga', 'running', 'sleep', 'stress', 'recovery', 'flexibility', 'balance', \n        'posture', 'hydration', 'motivation', 'diet', 'weight', 'training', 'gym', \n        'health', 'stretch', 'protein', 'calorie', 'endurance', 'aerobic', 'anaerobic'\n    ]\n\n    def __init__(self, model_path: str, max_length: int = 512):\n        \"\"\"\n        Initialize the fitness chatbot with a trained T5 model and tokenizer.\n        Args:\n            model_path (str): Path to the trained model directory.\n            max_length (int): Maximum token length for input/output.\n        \"\"\"\n        self.max_length = max_length\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        logger.info(f\"Using device: {self.device}\")\n        self.tokenizer = None\n        self.model = None\n        self.load_model(model_path)\n\n    def load_model(self, model_path: str) -> None:\n        \"\"\"\n        Load the T5 model and tokenizer from the specified path.\n        Args:\n            model_path (str): Path to the trained model directory.\n        \"\"\"\n        try:\n            if not os.path.exists(model_path):\n                raise FileNotFoundError(f\"Model path {model_path} does not exist.\")\n            logger.info(f\"Loading model from {model_path}\")\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n            self.model = T5ForConditionalGeneration.from_pretrained(model_path).to(self.device)\n            self.model.eval()\n            logger.info(\"Model and tokenizer loaded successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            sys.exit(1)\n\n    def clean_text(self, text: str) -> str:\n        \"\"\"\n        Clean input text by converting to lowercase, removing extra spaces, and special characters.\n        Args:\n            text (str): Input text to clean.\n        Returns:\n            str: Cleaned text.\n        \"\"\"\n        if not isinstance(text, str):\n            return \"\"\n        text = text.lower().strip()\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n        return text\n\n    def is_fitness_related(self, question: str) -> bool:\n        \"\"\"\n        Check if the question is fitness-related based on keywords.\n        Args:\n            question (str): User's question.\n        Returns:\n            bool: True if fitness-related, False otherwise.\n        \"\"\"\n        question = self.clean_text(question)\n        if not question or len(question.split()) < 3:\n            return False\n        pattern = '|'.join(self.FITNESS_KEYWORDS)\n        return bool(re.search(pattern, question, re.IGNORECASE))\n\n    def format_prompt(self, question: str, prompt_type: str = \"instruct\") -> str:\n        \"\"\"\n        Format the input question as a prompt for the T5 model.\n        Args:\n            question (str): User's question.\n            prompt_type (str): Type of prompt formatting ('instruct' or 'qa').\n        Returns:\n            str: Formatted prompt.\n        \"\"\"\n        if prompt_type == \"instruct\":\n            return f\"Answer this fitness-related question with specific advice: {question}\"\n        elif prompt_type == \"qa\":\n            return f\"question: {question}\\nanswer:\"\n        return question\n\n    def generate_response(self, question: str) -> str:\n        \"\"\"\n        Generate a response to the user's question using the T5 model.\n        Args:\n            question (str): User's question.\n        Returns:\n            str: Generated response or error/rejection message.\n        \"\"\"\n        try:\n            # Clean and validate input\n            question = self.clean_text(question)\n            if not question or len(question.split()) < 3:\n                return \"Please ask a valid question with at least a few words.\"\n\n            # Check if question is fitness-related\n            if not self.is_fitness_related(question):\n                return \"Sorry, I can only answer fitness-related questions. Please ask about exercise, nutrition, or health!\"\n\n            # Format prompt and tokenize\n            prompt = self.format_prompt(question)\n            inputs = self.tokenizer(\n                prompt,\n                return_tensors=\"pt\",\n                max_length=self.max_length,\n                truncation=True\n            ).to(self.device)\n\n            # Generate response\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_length=64,\n                    num_beams=5,\n                    no_repeat_ngram_size=2,\n                    early_stopping=True\n                )\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\n            # Fallback for vague responses\n            if len(response.split()) < 5 or \"sorry\" in response.lower():\n                return \"I couldn't provide a detailed answer. Try rephrasing or asking something like 'What are good protein sources for muscle gain?'\"\n\n            return response\n\n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\")\n            return \"An error occurred while generating the response. Please try again.\"\n\n    def run(self) -> None:\n        \"\"\"\n        Run the interactive chatbot loop, allowing users to type questions.\n        \"\"\"\n        print(\"\\n=== Fitness Chatbot ===\")\n        print(\"I'm here to answer fitness-related questions about exercise, nutrition, and health!\")\n        print(\"Type 'exit' or 'quit' to stop.\")\n        print(\"Examples: 'How can I improve my running endurance?' or 'What should I eat before a workout?'\")\n        print(\"Note: I only respond to fitness-related questions.\\n\")\n\n        while True:\n            try:\n                user_input = input(\"You: \").strip()\n                if user_input.lower() in ['exit', 'quit']:\n                    print(\"Goodbye!\")\n                    break\n\n                response = self.generate_response(user_input)\n                print(f\"Bot: {response}\\n\")\n\n            except KeyboardInterrupt:\n                print(\"\\nInterrupted. Goodbye!\")\n                break\n            except Exception as e:\n                logger.error(f\"Error in chatbot loop: {e}\")\n                print(\"An error occurred. Please try again.\\n\")\n\ndef main():\n    # Define model path (update if needed)\n    model_path = \"/kaggle/working/processed_data/fitness_qa_model_best_model_1\"\n    \n    # Initialize and run chatbot\n    chatbot = FitnessChatbot(model_path)\n    chatbot.run()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:04:50.457173Z","iopub.execute_input":"2025-06-19T12:04:50.457683Z","execution_failed":"2025-06-19T15:14:55.185Z"}},"outputs":[{"name":"stdout","text":"\n=== Fitness Chatbot ===\nI'm here to answer fitness-related questions about exercise, nutrition, and health!\nType 'exit' or 'quit' to stop.\nExamples: 'How can I improve my running endurance?' or 'What should I eat before a workout?'\nNote: I only respond to fitness-related questions.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what is agriculuture\n"},{"name":"stdout","text":"Bot: Sorry, I can only answer fitness-related questions. Please ask about exercise, nutrition, or health!\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:   how can i optimize my nutrition to support my fitness goals?\n"},{"name":"stdout","text":"Bot: maximizing your nutrition to support your fitness goals is a great way to improve your health and well-being.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  why should I optimize my nutrition to support my fitness goals?\n"},{"name":"stdout","text":"Bot: you should optimize your nutrition to support your fitness goals.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  are a gay?\n"},{"name":"stdout","text":"Bot: Sorry, I can only answer fitness-related questions. Please ask about exercise, nutrition, or health!\n\n","output_type":"stream"}],"execution_count":null}]}